{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tocom242242/notebooks/blob/master/pytorch/seg/simple_unet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC0tDeKsmzhA",
        "outputId": "f4af6976-99e3-424f-d659-487041538c49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn((1,3,256,256))\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNj5D7iopjsG",
        "outputId": "c4ca6b89-39a8-468c-aa5c-7fa309a20a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input x.shape: torch.Size([1, 3, 256, 256])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DownConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool= nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool(x)\n",
        "        return x\n",
        "\n",
        "class UpConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up_conv1 = nn.ConvTranspose2d(in_channels, out_channels, 3, stride=2, padding=1, output_padding=1)\n",
        "        self.conv1 = nn.Conv2d(out_channels*2, out_channels, 3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x = self.up_conv1(x1)\n",
        "        x = self.relu(x)\n",
        "        x = torch.cat([x, x2], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.down_conv1 = DownConv(in_channels, features[0])\n",
        "        self.down_conv2 = DownConv(features[0], features[1])\n",
        "        self.down_conv3 = DownConv(features[1], features[2])\n",
        "        self.down_conv4 = DownConv(features[2], features[3])\n",
        "        self.up_conv1 = UpConv(features[3], features[2])\n",
        "        self.up_conv2 = UpConv(features[2], features[1])\n",
        "        self.up_conv3 = UpConv(features[1], features[0])\n",
        "        self.out_upconv = nn.ConvTranspose2d(features[0], out_channels, 3, stride=2, padding=1, output_padding=1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.down_conv1(x)\n",
        "        x2 = self.down_conv2(x1)\n",
        "        x3 = self.down_conv3(x2)\n",
        "        x4 = self.down_conv4(x3)\n",
        "        x = self.up_conv1(x4, x3)\n",
        "        x = self.up_conv2(x, x2)\n",
        "        x = self.up_conv3(x, x1)\n",
        "        x = self.out_upconv(x)\n",
        "        return x\n",
        "\n",
        "print(\"input x.shape:\",x.shape)\n",
        "model = UNet(in_channels=3, out_channels=1, features=[64, 128, 256, 512])\n",
        "model(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgV28Dv1LhnK",
        "outputId": "d813e534-8cde-4a94-a3cb-c55ba7fe3d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./data/VOCtrainval_11-May-2012.tar\n",
            "Extracting ./data/VOCtrainval_11-May-2012.tar to ./data\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.transforms as T\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.datasets import VOCSegmentation\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "# 同時に画像とマスクに変換を適用するためのヘルパークラス\n",
        "class ComposeJoint(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "class ResizeJoint(object):\n",
        "    def __init__(self, size):\n",
        "        self.size = size\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        image = F.resize(image, self.size, Image.BILINEAR)\n",
        "        target = F.resize(target, self.size, Image.NEAREST)\n",
        "        return image, target\n",
        "\n",
        "# トランスフォームの定義\n",
        "transform = ComposeJoint([\n",
        "    ResizeJoint((256, 256)),\n",
        "    # lambda image, target: (T.ToTensor()(image), torch.as_tensor(np.array(target), dtype=torch.int64))\n",
        "    lambda image, target: (T.ToTensor()(image), T.ToTensor()(target))\n",
        "])\n",
        "\n",
        "# VOCデータセットの読み込み\n",
        "dataset = VOCSegmentation(root='./data', year='2012', image_set='train', download=True, transforms=transform)\n",
        "\n",
        "# DataLoaderの作成\n",
        "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tz1xmYBcLxOj"
      },
      "outputs": [],
      "source": [
        "# データの確認\n",
        "# for images, labels in data_loader:\n",
        "#     print(images.shape, labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: modelをgpuに載せて\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "TFoCV-Du6-2Z"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a, b = next(iter(data_loader))\n",
        "torch.unique(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdJAe00c7h6g",
        "outputId": "d833d918-090a-4007-eda9-a65656f4de26"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0000, 0.0235, 0.0275, 0.0314, 0.0627, 0.0745, 1.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM7Ft1wxL1Of",
        "outputId": "6f763112-6487-46b2-ef6d-10e4a81bfe0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss: 74.68472290039062\n",
            "Epoch 1: Loss: 85.310302734375\n",
            "Epoch 2: Loss: 113.46865844726562\n",
            "Epoch 3: Loss: 66.2655029296875\n",
            "Epoch 4: Loss: 46.95069885253906\n",
            "Epoch 5: Loss: 116.72239685058594\n",
            "Epoch 6: Loss: 138.3501434326172\n",
            "Epoch 7: Loss: 91.05636596679688\n",
            "Epoch 8: Loss: 62.99249267578125\n",
            "Epoch 9: Loss: 50.462982177734375\n",
            "Epoch 10: Loss: 103.19679260253906\n",
            "Epoch 11: Loss: 56.5571174621582\n",
            "Epoch 12: Loss: 100.02284240722656\n",
            "Epoch 13: Loss: 61.33900451660156\n",
            "Epoch 14: Loss: 40.6544189453125\n",
            "Epoch 15: Loss: 61.72590255737305\n",
            "Epoch 16: Loss: 136.92759704589844\n",
            "Epoch 17: Loss: 74.5814208984375\n",
            "Epoch 18: Loss: 89.07098388671875\n",
            "Epoch 19: Loss: 63.640045166015625\n",
            "Epoch 20: Loss: 107.43726348876953\n",
            "Epoch 21: Loss: 62.74976348876953\n",
            "Epoch 22: Loss: 61.31978225708008\n",
            "Epoch 23: Loss: 84.89266967773438\n",
            "Epoch 24: Loss: 74.0421371459961\n",
            "Epoch 25: Loss: 67.83377075195312\n",
            "Epoch 26: Loss: 86.34810638427734\n",
            "Epoch 27: Loss: 154.90670776367188\n",
            "Epoch 28: Loss: 77.73731231689453\n",
            "Epoch 29: Loss: 64.25047302246094\n",
            "Epoch 30: Loss: 47.914459228515625\n",
            "Epoch 31: Loss: 123.35303497314453\n",
            "Epoch 32: Loss: 113.52821350097656\n",
            "Epoch 33: Loss: 66.38066864013672\n",
            "Epoch 34: Loss: 51.45403289794922\n",
            "Epoch 35: Loss: 78.90170288085938\n",
            "Epoch 36: Loss: 46.364051818847656\n",
            "Epoch 37: Loss: 84.77555847167969\n",
            "Epoch 38: Loss: 107.66720581054688\n",
            "Epoch 39: Loss: 62.97552490234375\n",
            "Epoch 40: Loss: 103.23722076416016\n",
            "Epoch 41: Loss: 59.753875732421875\n",
            "Epoch 42: Loss: 38.818641662597656\n",
            "Epoch 43: Loss: 79.47189331054688\n",
            "Epoch 44: Loss: 58.92770767211914\n",
            "Epoch 45: Loss: 130.3890380859375\n",
            "Epoch 46: Loss: 67.23839569091797\n",
            "Epoch 47: Loss: 81.45909118652344\n",
            "Epoch 48: Loss: 124.9566650390625\n",
            "Epoch 49: Loss: 115.18360900878906\n"
          ]
        }
      ],
      "source": [
        "# prompt: 上で読み込んだデータセットを先ほど定義したUNetを学習するコードを書いて\n",
        "\n",
        "# 損失関数の定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 最適化手法の定義\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(50):\n",
        "    for images, labels in data_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # モデルの出力を計算\n",
        "        outputs = model(images)\n",
        "        outputs = outputs.squeeze(1)\n",
        "        labels = torch.squeeze(labels, 1)\n",
        "\n",
        "        # 損失を計算\n",
        "        loss = criterion(outputs,labels.float())\n",
        "\n",
        "        # 勾配の計算\n",
        "        loss.backward()\n",
        "\n",
        "        # パラメータの更新\n",
        "        optimizer.step()\n",
        "\n",
        "    # エポックごとに損失を表示\n",
        "    print('Epoch {}: Loss: {}'.format(epoch, loss.item()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYew3yNZozzX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features[0], 3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(features[0], features[1], 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(features[1], features[2], 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(features[2], features[3], 3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(features[3], features[2], 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(features[2], features[1], 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(features[1], features[0], 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(features[0], out_channels, 3, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "model = UNet()\n",
        "model(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IF8RQkRkpGB1"
      },
      "outputs": [],
      "source": [
        "# prompt: create or load dataset for above unet\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_paths = os.listdir(os.path.join(root_dir, 'images'))\n",
        "        self.mask_paths = os.listdir(os.path.join(root_dir, 'masks'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = os.path.join(self.root_dir, 'images', self.image_paths[idx])\n",
        "        mask_path = os.path.join(self.root_dir, 'masks', self.mask_paths[idx])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "        return image, mask\n",
        "\n",
        "train_dataset = CustomDataset('path/to/train_dataset')\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2YCXZlhnm8G"
      },
      "outputs": [],
      "source": [
        "# prompt: シンプルなU-Netを作成して\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, features[0], 3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(features[0], features[1], 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(features[1], features[2], 3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(features[2], features[3], 3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(features[3], features[2], 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(features[2], features[1], 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(features[1], features[0], 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(features[0], out_channels, 3, stride=1, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "model = UNet()\n",
        "model(x).shape"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMTn1ZSJB8bvb/+8zsmTQ6w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}